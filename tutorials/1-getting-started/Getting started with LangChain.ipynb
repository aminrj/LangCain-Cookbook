{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac97fdd5-0f1e-40a9-aef9-17eca6dba5f9",
   "metadata": {},
   "source": [
    "# Getting started with LangChain\n",
    "## What is LangChain\n",
    "LangChain enables building applicaitons that connect with external sources of data to be used with LLMs. \n",
    "LangChain libraries are made of Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
    "\n",
    "## Installation\n",
    "The easiest way to start with LangChain is usinga Jupyter Notebook.\n",
    "[Installation guide](https://python.langchain.com/docs/get_started/installation)\n",
    "\n",
    "To install LangChain with pip, use :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d11e82-4edd-448d-8c62-1fadfd2fd121",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af63ea53-6943-47c2-b58d-a49cc79d490c",
   "metadata": {},
   "source": [
    "## LLM Chain\n",
    "For this guide we will be using local open-source large language model.\n",
    "\n",
    "Ollama allows us to run sevaral open-source large language models, such as Llama 2, Mistral or Gemma, locally.\n",
    "\n",
    "First we need to install Ollama on our local machine, then fetch an LLM:\n",
    "- [Dowload Ollama](https://ollama.ai/download)\n",
    "- Fetch a model with `ollama pull llama2`.\n",
    "\n",
    "Then, make sure the Ollama server is running. After that, we can start our journey with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f46f60-c6ab-4284-9063-52221255ead5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b5ceeee-580f-49d5-a866-241be417c6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nLangChain is a set of reusable, modular components for building conversational interfaces. It was developed by the Language and Media Lab at Carnegie Mellon University, and is designed to make it easier to create chatbots and other conversational systems that are both effective and efficient.\\n\\nThe LangChain components are designed to be highly customizable, allowing developers to easily integrate them into their own projects. They also come with a range of pre-built functionality, such as natural language processing (NLP) capabilities and integration with popular messaging platforms like Facebook Messenger and Slack.\\n\\nSome of the key LangChain components include:\\n\\n1. Conversation State Machine: This component provides a way to manage the conversation state, including tracking the user's input and responding appropriately.\\n2. Natural Language Processing (NLP): This component includes tools for processing and understanding natural language inputs, such as sentiment analysis, entity recognition, and intent detection.\\n3. Dialogue Management: This component provides a way to manage the conversation flow, including handling user input, responding with appropriate messages, and escalating to a human operator if necessary.\\n4. Intent Classification: This component helps identify the user's intent based on their input, such as booking a flight or making a reservation.\\n5. Entity Recognition: This component identifies entities in the user's input, such as names, dates, and locations.\\n6. Sentiment Analysis: This component analyzes the user's sentiment, such as positive, negative, or neutral, based on their input.\\n7. Machine Learning: This component provides a way to integrate machine learning models into the conversational interface, allowing it to learn and improve over time.\\n8. Integration with Messaging Platforms: This component allows the conversational interface to be integrated with popular messaging platforms like Facebook Messenger, Slack, and more.\\n\\nBy using LangChain components, developers can create conversational interfaces that are more effective and efficient, allowing them to focus on building innovative features and experiences rather than managing complex conversation logic.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is LangChain components ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6296ffa9-762e-4e87-bc42-b7a1bc0fdaa3",
   "metadata": {},
   "source": [
    "We can guide it's response with a prompt template.\n",
    "Prompt templates are used to convert raw user input to a better input to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece8d210-db2f-42e4-b1a0-823dc77e9ece",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41182169-a994-4fa7-99c4-a16dd5392eb5",
   "metadata": {},
   "source": [
    "We combine these into a simple LLM chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "625bf518-40e9-411a-b559-2593f4e2f4fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c263fe45-bb00-4102-b5e9-d02bab81554a",
   "metadata": {},
   "source": [
    "We invoke the llm using the chain and ask the same question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f963147-408d-47ad-bff1-c4a5a367aedd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAs a world-class technical documentation writer, I'm happy to help you understand the components of LangChain!\\n\\nLangChain is an open-source platform designed to help developers create, manage, and maintain their technical documentation. It provides a modular architecture that enables teams to structure their documentation in a hierarchical manner, making it easier for users to find the information they need. Here are some of the key components of LangChain:\\n\\n1. Components: LangChain is built around components, which are self-contained pieces of content that can be combined to create more complex documents. Components can include anything from simple text blocks to interactive simulations and videos.\\n2. Pages: Pages are the building blocks of LangChain documents. They can contain any number of components and can be organized into sections, chapters, or other logical groupings.\\n3. Sections: Sections are larger units of content that can include multiple pages. They provide a way to organize related content under a single heading.\\n4. Chapters: Chapters are the highest level of organization in LangChain documents. They can contain any number of sections and provide a comprehensive overview of a particular topic or feature.\\n5. Categories: Categories are used to group related content together. They can be used to organize documentation by product, feature, or functionality.\\n6. Tags: Tags are used to label and organize content based on specific keywords or themes. They provide a way to search for and discover relevant content within LangChain.\\n7. Assets: Assets are any files or multimedia content that can be included in LangChain documents. This can include images, videos, audio clips, and other types of media.\\n8. Templates: Templates are pre-designed layouts that can be used to create new documents. They provide a way to standardize the structure and design of LangChain documents, making it easier for teams to create consistent content.\\n9. Plugins: Plugins are additional features or functionality that can be added to LangChain. These can include things like integration with other tools or services, or customizable workflows and processes.\\n10. User Management: LangChain provides a user management system that allows administrators to manage access to the platform, assign roles and permissions to users, and track usage and activity within the platform.\\n\\nThese are just some of the key components of LangChain, but I'm sure there is more to learn! Do you have any specific questions about any of these elements?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What are LangChain components?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aac278d-db23-4a98-85d9-2a0198ae179d",
   "metadata": {},
   "source": [
    "We can add a simple output parser to convert the chat message to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15af7e83-9299-4fb0-ae0f-a03165413845",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parsers = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ffb5c4-565e-4d55-b644-d4b6cb06f968",
   "metadata": {},
   "source": [
    "We can now add this to the previous chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7e8fb33-3d12-4018-878b-0856a98a89a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d453fb6e-1709-4b5d-beab-86cfe9dbcd12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAs a world-class technical documentation writer, I'm glad you asked! LangChain is a powerful tool for creating and managing technical documentation, and it consists of several components that work together to provide a seamless documentation experience. Here are the main components of LangChain:\\n\\n1. Documents: This is where all your technical documentation lives. You can create, edit, and manage documents in LangChain, and they can be organized into folders and categories for easy navigation.\\n2. Pages: Within each document, you can create pages that contain the content you want to document. Pages are like chapters or sections within a larger document, and they can be nested within other pages for a hierarchical structure.\\n3. Items: LangChain also allows you to create items, which are smaller pieces of content that can be used throughout your documentation. Items can be anything from code snippets to images, and they can be easily inserted into pages or other items.\\n4. Relationships: One of the key features of LangChain is its ability to create relationships between items and pages. This allows you to create a hierarchy of content that is easy to navigate and understand. For example, you can create a page that links to several related items, or an item that links to multiple pages that provide more detail on a particular topic.\\n5. Templates: LangChain provides a range of templates that you can use to create documentation that follows a consistent structure. Templates can be customized to fit your organization's specific needs, and they can help ensure that your documentation is professional and well-organized.\\n6. Workflow: LangChain also includes a workflow feature that allows you to assign tasks and track progress on documentation projects. This can help ensure that everyone involved in the project is on the same page and that tasks are completed efficiently.\\n7. Integration: Finally, LangChain integrates with a range of other tools and systems, including version control systems like Git, project management tools like Jira, and content management systems like WordPress. This makes it easy to incorporate documentation into your existing workflow and ensure that everything is connected and streamlined.\\n\\nOverall, the components of LangChain work together to create a powerful and flexible technical documentation system that can help you manage and organize your content in a way that is efficient and easy to use.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"input\": \"What are LangChain components?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304335dd-1977-4f4c-9c7d-75b53ffa0d7a",
   "metadata": {},
   "source": [
    "Now, we have successfully set the basic LLM chain.\n",
    "\n",
    "However, in order to properly answer this question, we need to provide additional context to the LLM.\n",
    "\n",
    "This is where the **Retrieval Chain** comes into play.\n",
    "\n",
    "## Retrieval Chain\n",
    "\n",
    "Retrieval is useful when you have too much data to pass to the LLM directly. You can then use a retriever to fetch only the most relevant pieces and pass them in.\n",
    "\n",
    "A retriever can be backed by any source of information : SQL database, PDF documents, Internet...\n",
    "\n",
    "However, these information needs to be translated and stored into a **vector store** to be used by the retriever.\n",
    "\n",
    "Let's see how to do it:\n",
    "\n",
    "We will be using the Internet as our source by loading information from the \"langchain docs\" website.\n",
    "For this, we use the `BeautifulSoup` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "479af370-b5c3-49ac-b401-2580d2f0947c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install beautifulsoup4 -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1a9ab8b-efbc-4110-b3a3-ddb18b5d55a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a67e5d-3e7a-4705-8387-f51dca020b94",
   "metadata": {},
   "source": [
    "Next, we need to index it into a vectorstore.\n",
    "This requires a few components: an [embedding model](https://python.langchain.com/docs/modules/data_connection/text_embedding) and a [vectorstore](https://python.langchain.com/docs/modules/data_connection/vectorstores).\n",
    "\n",
    "For embedding models, we use our Ollama Embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62b96e99-552b-4422-8cb4-eba05640d6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e22f52-4712-4cc1-bdfd-da19b1e75181",
   "metadata": {},
   "source": [
    "Next, once we have our embedding, we need to ingest our documentation into a vectorstore.\n",
    "We will use a simple local vectorstore, [FAISS](https://python.langchain.com/docs/integrations/vectorstores/faiss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5a9a555-f29d-4912-9473-084094588887",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /Users/ARAJI/miniconda3/lib/python3.11/site-packages (1.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda0e4ae-a9d7-4fd1-bc74-6e895abefcf0",
   "metadata": {},
   "source": [
    "Then, we build our index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f3c5deb-aff0-48bc-92e4-7191d4ea29eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769de65-925c-43b6-bea4-2bd781869b47",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now the data is indexed into a vectorstore, we will create a retrieval chain.\n",
    "This chain will take an incoming question, look ut relevant documents, then pass those documents along with the original question into our LLM and ask it to answer the original question.\n",
    "\n",
    "First, let's set up the chain that takes the qestion and the retrieved documents and generates an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dec7c102-e592-458b-9df0-2e7674611b07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\" Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c17401-c93d-47ff-8e50-ee65a1bb3ed5",
   "metadata": {},
   "source": [
    "We need to create a document retriever from our vector.\n",
    "Then use that retriever in a retriever chain to ask our question with the provided context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41766dcf-6c97-454e-9a0a-4b916b978439",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bcb039-ade4-4212-9b42-c99cf2c6c79c",
   "metadata": {},
   "source": [
    "Now we can invoke this chain. \n",
    "It should return a response from the LLM as a dictionary with the `answer` as key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bec5e11a-94d6-4a4e-8711-dc807d1818d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"What are LangChain components?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d35c4f6-4f35-49e0-b121-60659be9baa1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What are LangChain components?',\n",
       " 'context': [Document(page_content='ðŸ¦œï¸ðŸ”— Langchain', metadata={'source': 'https://python.langchain.com/docs/', 'title': 'ðŸ¦œï¸ðŸ”— Langchain', 'language': 'en'})],\n",
       " 'answer': 'ðŸ“ Based on the provided context, LangChain components refer to the building blocks of a LangChain network. These components are the individual elements that make up the network and enable it to perform its intended function. ðŸ”—'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833add9-cdcf-480d-8b82-ca6bb02c3af5",
   "metadata": {},
   "source": [
    "The answer is now more accurate.\n",
    "\n",
    "Now, that we have our LLM using personalised context and prompt, we can start to touch the power of LangChain.\n",
    "However, we are barely touching the surface of what's possible.\n",
    "\n",
    "In our example above, the LLM can only answer one question at a time with no context saved from the chat history.\n",
    "Next, we will build our own chat bot, capable to answer follow up questions, always using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464c68a3-07ed-432e-88c0-8af899584cbf",
   "metadata": {},
   "source": [
    "## Conversation Retrieval Chain\n",
    "To build our conversational retrival chain, we can reuse our `create_retrieval_chain` function, however, we need to change two things:\n",
    "1. the retrieval method should take the whole history into account\n",
    "2. the final LLM chain should likewise take the whole history into account\n",
    "\n",
    "### Update the Retriever chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16004465-0860-4a88-bbbe-3c98dfc236f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
    "])\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f31a1f-72e6-42cf-ac3a-08120aa17877",
   "metadata": {},
   "source": [
    "To test this, we pass in an an instance where the user is asking a follow up question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f96f94bd-5990-4465-aa3f-c1a96ca1eae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='ðŸ¦œï¸ðŸ”— Langchain', metadata={'source': 'https://python.langchain.com/docs/', 'title': 'ðŸ¦œï¸ðŸ”— Langchain', 'language': 'en'})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangChain help create personalised LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1b56f5-71f3-4ed2-b815-b6a6ec025331",
   "metadata": {},
   "source": [
    "This returns documents about using LangChain to personalise LLM applications.\n",
    "This is because the LLM generated a new query, combining the chat history with the follow up question.\n",
    "\n",
    "\n",
    "Now that we have this retriever, we can create a new chain to continue the conversation with these retriever documents in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8366af55-8653-42a5-bcc5-9ec276e0fc5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37395664-9a85-49eb-a92d-2596778d45e4",
   "metadata": {},
   "source": [
    "Now we test the new retriever chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42f4b9b5-8fe0-432a-89cc-cce4354e4575",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='Can LangChain help create personalised LLM applications?'),\n",
       "  AIMessage(content='Yes!')],\n",
       " 'input': 'Tell me how',\n",
       " 'context': [Document(page_content='ðŸ¦œï¸ðŸ”— Langchain', metadata={'source': 'https://python.langchain.com/docs/', 'title': 'ðŸ¦œï¸ðŸ”— Langchain', 'language': 'en'})],\n",
       " 'answer': \"Of course, I'd be happy to help you understand how LangChain can assist in creating personalized LL.M applications! ðŸ˜Š\\n\\nLangChain is an AI-powered language learning platform that utilizes natural language processing (NLP) and machine learning (ML) algorithms to analyze and generate human-like language outputs. By leveraging these technologies, LangChain can help create personalized LL.M applications tailored to individual learners' needs and goals. ðŸŽ¯\\n\\nHere are some ways LangChain can support the creation of personalized LL.M applications:\\n\\n1. **Customized learning paths**: With LangChain, you can create customized learning paths for each learner based on their language level, learning style, and areas of interest. The platform's AI-driven content creation capabilities allow it to generate a wide range of linguistically accurate and culturally relevant materials, ensuring that learners receive the most appropriate and engaging content.\\n2. ** Adaptive assessments**: LangChain's adaptive assessment features enable you to create customized quizzes and tests that adjust their level of difficulty and content based on each learner's performance. This provides a more accurate picture of each learner's language proficiency and helps identify areas where they need improvement.\\n3. **Personalized feedback**: LangChain's AI-powered feedback engine can provide learners with tailored feedback on their language use, highlighting areas of improvement and suggesting specific exercises to help them advance. This personalized approach helps learners more effectively identify and address their language learning needs.\\n4. **Collaborative learning**: LangChain's multiplayer features allow learners to interact with one another in a supportive and competitive environment, fostering collaboration and social learning. By leveraging the power of peer-to-peer learning, LangChain can help create a more engaging and effective language learning experience for each individual learner.\\n5. **Data-driven insights**: The platform's analytics dashboard provides valuable data insights into learners' progress, allowing you to track their improvement over time and identify areas where they may need additional support. This data can also be used to adjust the learning content and path for each learner, ensuring that their learning experience is tailored to their unique needs and goals.\\n\\nBy harnessing the power of LangChain's AI-driven language learning capabilities, you can create personalized LL.M applications that cater to individual learners' linguistic and cultural backgrounds, learning styles, and goals. This approach helps ensure that each learner receives a tailored learning experience that optimizes their language proficiency growth and overall educational success. ðŸš€\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangChain help create personalised LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16ed1af-6694-4e5b-b012-9735d082cf4c",
   "metadata": {},
   "source": [
    "This is a coherent answer. We successfuly turned our retrieval chain into a chatbot ðŸ‘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194f3f27-8b17-436b-b62c-ac08ad03c187",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f3fe7-c553-4396-abb8-bcc244feca1f",
   "metadata": {},
   "source": [
    "Up until now, we create our own steps while interacting with the LLM.\n",
    "\n",
    "With \"Agents\", we will let the LLM decides what steps to take.\n",
    "\n",
    "The first step with agents is to decide what tools the agent will have at its disposal to conduction the task.\n",
    "\n",
    "We will be using a simple self-ask with search agent.\n",
    "This agent will use a local LLM and a search engin to retrieve information.\n",
    "First, we initialize the tools we want to use.\n",
    "\n",
    "For this agent, only one tool can be used and it needs to be named \"Interediate Answer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a4a619c2-afc1-4425-8365-7e853f2acc8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b85cd3a8-1255-49e9-bd90-34c14e9ebf25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_self_ask_with_search_agent\n",
    "from langchain_community.llms import Fireworks\n",
    "from langchain_community.tools.tavily_search import TavilyAnswer\n",
    "\n",
    "# let's change our llm for his task\n",
    "llm = Ollama(model=\"mistral\")\n",
    "\n",
    "tools = [TavilyAnswer(max_results=1, name=\"Intermediate Answer\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f13abc-9d2f-40f2-a8ad-49791be2b842",
   "metadata": {},
   "source": [
    "To give our LLM access to a search engin, we will use [Tavily](). This wil require and API key (don't worry, the free tier is quite enough).\n",
    "After creating it on their platform, define a variable to store the key securely using getpass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a58ac-d595-471b-aec0-cb89f5f8e521",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8dbda-e655-44a6-b80d-338960f4aa44",
   "metadata": {},
   "source": [
    "Finally, once we have the tool, we create the agent that will use it.\n",
    "\n",
    "We install the `langchain hub` first, then we can use it to get a predefined prompt:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667ef765-6ba5-42ee-a23c-606ebb63064a",
   "metadata": {},
   "source": [
    "Get the prompt to use - you can modify this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "616937bc-53ac-44ce-bfdd-486bc62c4c94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"hwchase17/self-ask-with-search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05996c37-4a4f-4b5b-9bb5-00af093283a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = create_self_ask_with_search_agent(llm, tools, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22de0567-1e8a-484b-bc94-b364e090c76a",
   "metadata": {},
   "source": [
    "### Run the agent\n",
    "\n",
    "To run the agent, we create an agent executor by passing in the agent and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "792d4f4d-d96c-46bc-b2f7-349ea9b10711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10eeb0ad-59b6-4ef9-87d5-8b803805e68d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Yes, I can provide some context.\n",
      "\n",
      "Follow up: What is the total area of Morocco?\u001b[0m\u001b[36;1m\u001b[1;3mThe total area of Morocco is 446,550 square kilometers as of 2021, based on data from the World Bank collection of development indicators.\u001b[0m\u001b[32;1m\u001b[1;3m Follow up: What is the total area of Sweden?\u001b[0m\u001b[36;1m\u001b[1;3mThe total area of Sweden is approximately 450,295 square kilometers, as reported by the CIA World Factbook.\u001b[0m\u001b[32;1m\u001b[1;3m So the final answer is: Sweden is slightly larger than Morocco in terms of area. (Sweden's area is about 5,765 square kilometers larger)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Which country is bigger, Morocco or Sweden in term for area ?',\n",
       " 'output': \" Sweden is slightly larger than Morocco in terms of area. (Sweden's area is about 5,765 square kilometers larger)\"}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\"input\": \"Which country is bigger, Morocco or Sweden in term for area ?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5ffb00-907a-46cc-8842-9846f6c0a6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
